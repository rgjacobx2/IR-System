{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Specific IR System - Wiki Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Wiki specific IR System to retrieve relevant Wiki docs based on an input User Query.\n",
    "This code will will take a wiki file as input and perform the following:\n",
    "##### 1. Tokenize the wiki document\n",
    "##### 2. Apply stemming using porter stemmer\n",
    "##### 3. Generate an Inverted Index\n",
    "##### 4. Calculate TF-IDF Weights and Cosine Similarity\n",
    "##### 5. Return the top 10 document IDs from the input User Query\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Input : Wiki File (Path to be provided at line 17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import requests\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "\n",
    "#Provide the path of the wiki file here\n",
    "wikiPath = \"C:/Users/dnv786/Desktop/Zebra/Personal/Mtech/Semester2/IR/Assignment/Sem2Assignment/wiki_00\"\n",
    "\n",
    "indexFilePath = 'InvertedIndex.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. getUserQueryVector(userQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a user query input and generates a vector from the query\n",
    "This vector is then used to help in calculating Term Weights wrt Document terms in the Inverted Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserQueryVector(userQuery):\n",
    "    \"\"\"\n",
    "    This method calculates Term frequency from the document\n",
    "    \"\"\"\n",
    "    #First we tokenize the User Query just like how the wiki page was tokenized\n",
    "    ps = PorterStemmer()\n",
    "    queryTokens = nltk.word_tokenize(userQuery)\n",
    "    stemmedQueryTokens = [ps.stem(eachWord) for eachWord in queryTokens]\n",
    "#     queryTokens= [token.lower() for token in queryTokens if not re.match(r'^[_\\W]+$', token)]\n",
    "    cleanQueryTokens = []\n",
    "    for eachQueryToken in stemmedQueryTokens:\n",
    "        if not re.match(r'^[_\\W]+$',eachQueryToken): #Checks if Punctuations are not present like [. , \"''\"] etc\n",
    "            cleanQueryTokens.append(eachQueryToken.lower())\n",
    "    \n",
    "    queryTokenCount = {}\n",
    "    for eachQueryToken in cleanQueryTokens:\n",
    "        if eachQueryToken not in queryTokenCount:\n",
    "            queryTokenCount[eachQueryToken] = 1\n",
    "        else:\n",
    "            queryTokenCount[eachQueryToken] += 1\n",
    "\n",
    "    print(\"Query Tokens       : \" + str(cleanQueryTokens))\n",
    "    print(\"Query Token Count  : \" + str(queryTokenCount))\n",
    "    \n",
    "    \n",
    "    #Get the total number of documents in the wiki file. \n",
    "    #This we have already stored in the documentCount index in the InvertedIndex Data Structure\n",
    "    totalDocumentsInInvertedIndex = InvertedIndex['documentCount']\n",
    "    queryVector = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##THIS IS WHERE THE MAGIC HAPPENS\n",
    "    ## Term Weight = (1+log(tf)) x (log(N/docFrequency))\n",
    "     \n",
    "    for token, count in queryTokenCount.items():\n",
    "        if token in InvertedIndex[\"terms\"]:\n",
    "            \n",
    "            # Term Frequency = (1+log(tf))\n",
    "            termFrequency = 1 + math.log(count, 10)\n",
    "            documentFrequency = InvertedIndex[\"terms\"][token]['docs_count']\n",
    "            \n",
    "            # Inverse Document Frequency = log(N/docFrequency) , Where N - Total Documents in Inverted Index\n",
    "            inverseDocumentFrequency = math.log(totalDocumentsInInvertedIndex/documentFrequency, 10)\n",
    "            \n",
    "            ## Term Weight = Term Frequency x Inverse Document Frequency\n",
    "            termWeight = termFrequency * inverseDocumentFrequency\n",
    "            \n",
    "            ## Generate Query Vector\n",
    "            if token not in queryVector:\n",
    "                queryVector[token] = termWeight\n",
    "            else: \n",
    "                queryVector[token] += termWeight\n",
    "\n",
    "    return queryVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. retrieveTop10Docs(userQuery)\n",
    "\n",
    "##### This is the Top level method that feeds in the user query\n",
    "\n",
    "This will call the cosineScore and the getUserQueryVector methods to perform their corresponding operations.\n",
    "\n",
    "This will finally retrieve the top 10 matching document IDs corresponding to the input User Query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveTop10Docs(userQuery):\n",
    "    userQueryVector = getUserQueryVector(userQuery)\n",
    "    print(\"Query Vector : \" + str(userQueryVector))\n",
    "    scores = cosineScore(userQueryVector)\n",
    "    top10Count = 0\n",
    "    #Retrieve the first 10 documents from the document list that has the best scores matching the input sample query\n",
    "    print('Top 10 Documents that match the sample input query, with corresponding scores : ')\n",
    "    for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True):\n",
    "        print(k, v)\n",
    "        top10Count += 1\n",
    "        if top10Count >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. cosineScore(userQueryVector)\n",
    "This method takes the Vector generated from getUserQUeryVector method and then compares the cosine score between the user query and the document vector.\n",
    "\n",
    "This method calculates the cosine score between vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineScore(userQueryVector):\n",
    "    scores = {}\n",
    "    for token, qnorm in userQueryVector.items():\n",
    "        for doc, data in InvertedIndex[\"terms\"][token]['docs'].items():\n",
    "            score = qnorm * data['tf_norm']\n",
    "            if doc not in scores:\n",
    "                scores[doc] = score\n",
    "            else:\n",
    "                scores[doc] += score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. termFrequency(doc, term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method calculates Term frequency from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTermFrequency(doc, term):\n",
    "    if doc not in InvertedIndex[\"terms\"][term]['docs']: return 1\n",
    "    else:\n",
    "        count = InvertedIndex[\"terms\"][term]['docs'][doc]['count']\n",
    "        termFrequency = 1 + math.log(count, 10)\n",
    "    return termFrequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START OF MAIN PROGRAM\n",
    "\n",
    "First Tokenize the documents from the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Wiki File. This will take a few seconds...\n",
      "Reading Complete. Generated Beautiful soup object from Wiki File\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading Wiki File. This will take a few seconds...\")\n",
    "soup = BeautifulSoup(open(wikiPath, encoding=\"utf8\"), \"html.parser\")\n",
    "print(\"Reading Complete. Generated Beautiful soup object from Wiki File\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Index Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first define the structure for the Inverted Index which we need to create. \n",
    "\n",
    "The Beautiful Soup library is used to extract the data from any HTML/XML based files.\n",
    "\n",
    "We also have to find each document in the Wiki File.\n",
    "\n",
    "The Wiki File has a \"doc\" tag which We shall try to search in the Wiki File to identify each document.\n",
    "\n",
    "And then we save the Document ID of the \"doc\" element from the wiki document\n",
    "\n",
    "We will also be using Porter Stemmer operations to stem each token by creating a Porter Stemmer object\n",
    "\n",
    "### Note :  Executing the following will take a while. Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Tokens, Stemming and creating the Inverted Index.\n",
      "This will take a while. Please Wait till it says Complete...\n",
      "Tokenization, Stemming and Inverted Index Construction Complete\n"
     ]
    }
   ],
   "source": [
    "#Initializing the Inverted Index Structure\n",
    "InvertedIndex = {'documentCount' : 0 , 'terms': {}}\n",
    "\n",
    "#Create Porter Stemmer object for Stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(\"Generating Tokens, Stemming and creating the Inverted Index.\")\n",
    "print(\"This will take a while. Please Wait till it says Complete...\")\n",
    "\n",
    "for eachDocument in soup.find_all('doc'):\n",
    "    \n",
    "    InvertedIndex['documentCount']+=1\n",
    "    docID = int(eachDocument['id'])\n",
    "    \n",
    "    #Tokenize the Each document using  NLTK Tokenizer\n",
    "    documentTokens = nltk.word_tokenize(eachDocument.get_text())\n",
    "    \n",
    "    #Apply Stemming operation using Porter Stemmer\n",
    "    stemmedTokens = [ps.stem(eachWord) for eachWord in documentTokens]\n",
    "\n",
    "    #Bring all tokens to lower case\n",
    "    cleanTokens = []\n",
    "    for eachToken in stemmedTokens:\n",
    "        #Checks if Punctuations are not present like [. , \"''\"] etc\n",
    "        if not re.match(r'^[_\\W]+$',eachToken): \n",
    "            cleanTokens.append(eachToken.lower())\n",
    "    \n",
    "    for eachToken in cleanTokens:\n",
    "        if eachToken not in InvertedIndex['terms']:\n",
    "            #Initialize the Inverted Index structure for every New term\n",
    "            InvertedIndex['terms'][eachToken] = {'docs' : {docID : {'count':1,'tf':0}},'total' : 1, 'docs_count':1}\n",
    "        else:\n",
    "            if docID not in InvertedIndex['terms'][eachToken]['docs']:\n",
    "                InvertedIndex[\"terms\"][eachToken]['docs'][docID] = {'count':1,'tf': 0}\n",
    "                InvertedIndex[\"terms\"][eachToken]['docs_count'] +=1\n",
    "                InvertedIndex[\"terms\"][eachToken]['total'] +=1\n",
    "            else:\n",
    "                InvertedIndex[\"terms\"][eachToken]['docs'][docID]['count'] +=1\n",
    "                InvertedIndex[\"terms\"][eachToken]['total'] +=1\n",
    "                \n",
    "                \n",
    "\n",
    "print(\"Tokenization, Stemming and Inverted Index Construction Complete\")\n",
    "# print(InvertedIndex)\n",
    "# print(cleanTokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Term Frequency for each document in the wiki file represented by document ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Term Frequency for each document\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Term Frequency for each document\")\n",
    "for eachTerm in InvertedIndex[\"terms\"]:\n",
    "    for docID in InvertedIndex[\"terms\"][eachTerm]['docs']:\n",
    "        termFrequency = calculateTermFrequency(docID,eachTerm)\n",
    "        InvertedIndex[\"terms\"][eachTerm]['docs'][docID]['tf'] = termFrequency   \n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Document Term Frequency using Cosine Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{12: 61.875859443888146, 339: 52.505353112272786, 1023: 60.699152989882386, 25: 53.43784765878462, 39: 35.44412211157464, 290: 24.399519400571528, 303: 65.62818773801743, 305: 46.0532148714614, 307: 71.3852133094466, 308: 59.98122679110702, 309: 31.83072423580938, 316: 9.949820820695095, 324: 46.25194790156348, 330: 15.056443151316303, 332: 15.42739916403205, 334: 25.08520011949939, 336: 46.122755482729005, 340: 12.56747742179449, 344: 19.001173812856205, 358: 63.668811496658805, 359: 28.1154533068923, 569: 51.16606723308775, 572: 21.11996335643244, 573: 54.39452080499002, 580: 21.733382684246692, 586: 45.702928586802564, 593: 35.867354109104646, 594: 60.06713528318535, 595: 51.7463036265206, 597: 26.644473509724634, 599: 28.111489085655126, 600: 47.807783053496536, 612: 23.134068635871433, 615: 20.87935896341753, 620: 43.74191512815392, 621: 63.670273742499454, 624: 62.19346694605006, 627: 55.617313369608446, 628: 40.24756120917949, 633: 42.82645279526562, 634: 43.621228129506555, 639: 47.86277896498875, 640: 35.81650586087682, 642: 14.92086622308076, 643: 18.881271241239475, 649: 23.25048773315608, 651: 25.862929574041203, 653: 39.84694727888667, 655: 39.31272128666422, 656: 32.92723918953713, 657: 49.749969903711964, 659: 20.295825162728296, 662: 50.25172817018059, 663: 52.62918623368287, 664: 38.63256135341141, 665: 32.42837248017978, 666: 66.27373992104815, 670: 43.22356502078942, 673: 27.19673927490389, 674: 50.65665900058386, 675: 14.261785578909913, 676: 43.60470199013572, 677: 39.781240651489156, 679: 4.501030467063201, 680: 38.83503718236752, 681: 31.848035830113393, 682: 32.99685782169774, 683: 16.381422504716443, 689: 51.46880632608945, 690: 40.473895448672, 691: 46.5510300211201, 694: 5.1223374208510695, 696: 5.975038645613315, 698: 35.51402209515498, 700: 54.48749981388625, 701: 57.574658402647955, 704: 24.544293786430256, 705: 24.165032061316584, 706: 36.90910884667677, 708: 13.206136908862026, 709: 31.24070417436805, 710: 28.5439997656081, 711: 42.12951049163186, 713: 33.84852240203519, 717: 57.93447224782085, 734: 20.391817980779702, 736: 61.104234893198985, 737: 62.546388062158236, 738: 68.78583148427722, 740: 33.008579438151145, 742: 7.325638126231238, 746: 65.87110107213314, 748: 32.932602229008644, 751: 40.43580965695843, 752: 53.14421031032461, 764: 18.13036809668172, 765: 50.61431026827465, 766: 14.33494788369191, 771: 69.22435761249656, 772: 21.669107164702254, 775: 50.0339073113234, 777: 19.398442078445914, 779: 9.693659744100042, 780: 3.6306868773201293, 782: 40.17420260817785, 783: 62.94031802636745, 784: 26.643154924846893, 785: 37.623951408631, 786: 40.92899635228878, 787: 19.134974182902265, 788: 12.676246005362891, 789: 22.601295112553927, 791: 49.17299109036607, 794: 16.600723959254122, 795: 21.990632239596934, 798: 41.97291530229179, 799: 35.925635835454706, 800: 45.0940693708758, 802: 57.224142873213154, 803: 56.83557230271232, 808: 62.768914778195686, 809: 15.205696671631046, 824: 39.7758177667326, 825: 29.106583346039184, 840: 35.86130102344701, 841: 47.62884715446907, 842: 26.20913507117338, 843: 40.47091167870452, 844: 66.00714427296712, 846: 5.515218174826185, 848: 62.25080512497568, 849: 38.290766181502335, 851: 34.74288355768108, 852: 60.44908470377118, 854: 39.67832060855016, 856: 68.09018509299892, 857: 29.174310431610095, 859: 16.33021999191561, 863: 66.45379696052751, 864: 56.219135288399585, 868: 28.334818222978775, 869: 27.748540659515026, 872: 72.4939947178649, 874: 68.6218927983129, 876: 12.289266697887104, 877: 24.9754779447224, 878: 35.04142272853236, 880: 65.90675950486532, 881: 33.285701422492345, 887: 35.351674011470394, 888: 32.97370503827713, 890: 39.89693175637921, 894: 34.98344733033166, 896: 35.88034318037806, 897: 53.73394969720712, 898: 42.05123379752744, 899: 29.712574767218594, 900: 47.807274340188016, 901: 44.78753740059499, 902: 51.16298851172042, 903: 19.206941835468935, 904: 50.899763438631815, 905: 28.33063067039611, 909: 36.561662647973314, 910: 9.447342638204324, 911: 14.862788032778788, 914: 32.82872314822162, 915: 21.286177835090804, 921: 19.53083208370049, 922: 39.536253759354494, 924: 34.52191805647175, 925: 16.18597462699612, 928: 38.58888079991238, 929: 20.645392513695683, 930: 34.18376054451564, 931: 44.936525795161984, 951: 40.3388261340646, 953: 17.867659704450272, 954: 54.348519223062944, 956: 36.83040176991689, 957: 26.15954731003291, 958: 35.94000760665568, 960: 26.614418650075585, 966: 13.955194751287845, 967: 32.5322902261149, 969: 35.55695022083611, 972: 29.989231454002205, 974: 43.496648189631, 980: 42.10992143050138, 981: 57.666259167587384, 983: 44.91730621434639, 984: 54.25104355746392, 986: 30.746685652474827, 988: 17.264721796496897, 991: 24.70573561079148, 993: 17.68791357367697, 994: 10.170166544528, 1000: 53.95961054046459, 1002: 35.53324837636957, 1004: 22.899295775061233, 1005: 16.286803821358724, 1006: 43.288407350205, 1014: 39.104661876525405, 1016: 40.35390247876909, 1017: 58.73279002125023, 1018: 22.274786248849182, 1020: 37.70637023594451, 1021: 13.705723140553966, 1022: 42.09820798187708, 1028: 44.38854920566356, 1029: 52.49323134534816, 1030: 44.683044886363604, 1032: 31.04531602081848, 1036: 16.037037094941, 1038: 64.45922281266128, 1043: 9.411611238295002, 1049: 5.842472986737625, 1051: 41.51455094562812, 1055: 28.74449627645473, 1057: 21.90389954851158, 1058: 31.798136815472617, 1063: 29.339736026985353, 1064: 41.17860090044236, 1069: 13.555035852282362, 1070: 20.671237246500322, 1072: 9.814947444373496, 1074: 9.996711625110695, 1078: 70.90576097916784, 1081: 35.64318571102439, 1082: 24.8908575981298, 1087: 21.65518252815736, 1088: 43.5627759278185, 1091: 25.246882026401543, 1092: 17.708519148475407, 1093: 37.063459841054474, 1094: 53.6173994454076, 1096: 10.687416587729295, 1097: 41.02826137375012, 1098: 26.150993595639594, 1109: 10.054025644072437, 1110: 6.5419762645853785, 1111: 17.42747286859268, 1112: 19.166005885685376, 1130: 51.95427285738199, 1132: 56.62783731891544, 1134: 25.56843396991721, 1135: 34.92845390327887, 1136: 8.613436199899814, 1140: 37.71494964787378, 1141: 23.011586139421123, 1143: 43.57528457916931, 1144: 33.302680774325744, 1146: 35.934033108424074, 1148: 59.21891673020882, 1152: 38.65211572799288, 1158: 16.842303251509165, 1160: 17.459136572464438, 1162: 39.060449001962056, 1164: 55.30021090303563, 1166: 28.417664025528126, 1167: 25.440685334587844, 1168: 41.55382765007975, 1170: 35.331564456728536, 1171: 34.233518244899116, 1174: 37.74147248034826, 1176: 13.223895564845211, 1177: 64.04342989713, 1178: 61.76686545494949, 1181: 26.54246524807787, 1182: 52.105324045344155, 1183: 33.022402024237714, 1187: 38.356235306602564, 1192: 39.8093325540845, 1193: 25.382231772099857, 1196: 30.47942824467057, 1198: 34.55770392137956, 1200: 22.760027990570116, 1201: 42.766477093054476, 1202: 20.659712686331336, 1203: 56.13958745303311, 1206: 48.49973257899845, 1207: 44.03867476631666, 1208: 55.215160225556936, 1209: 33.26426449320139, 1210: 37.54977759029055, 1212: 22.705427804649833, 1213: 27.136094179170563, 1214: 65.1256734395241, 1216: 57.80756118609904, 1217: 42.513356223366216, 1223: 9.452255083348383, 1227: 26.580333486843365, 1234: 22.71037860183567, 1235: 23.0195607195374, 1239: 48.221108174895825, 1241: 33.85833279402616, 1242: 38.804834806289094, 1247: 27.45453800131675, 1252: 40.354074564495356, 1256: 46.57866263826234, 1260: 38.30410843112842, 1262: 15.147266804250249, 1264: 28.19039782383725, 1267: 31.73145972300304, 1270: 46.616051228757875, 1271: 31.32387631155891, 1273: 67.99043584766234, 1274: 18.303776806643057, 1279: 29.583656976568946, 1285: 30.627795121372653, 1286: 20.91696357985511, 1288: 43.2589495159844, 1291: 27.121581351905863, 1293: 26.486020853763865, 1298: 35.604950349591526, 1300: 40.735553390813465, 1301: 26.82119636209968, 1303: 10.230551293342895, 1305: 28.87099315109969, 1306: 38.5013661958513, 1307: 31.778021130940367, 1309: 13.562856110119014, 1313: 25.2597266601621, 1315: 40.64674717665632, 1316: 37.357324469082094, 1317: 40.696747017230116, 1322: 31.614931369667666, 1324: 26.728289971120663, 1325: 42.247165999656325, 1327: 29.620068909070373, 1331: 13.27958503275897, 1335: 21.576898741676544, 1336: 20.45356809459664, 1338: 47.90277563970437, 1344: 22.075619635586932, 1346: 46.62733897139802, 1347: 53.00173105735548, 1348: 46.583543435213066, 1349: 29.478876001675925, 1354: 41.133315075940644, 1356: 7.355159939843523, 1358: 43.92526370825103, 1359: 15.750122129645689, 1360: 24.04656052720456, 1361: 34.77188250153464, 1362: 18.513206861473456, 1363: 27.524920564250227, 1365: 55.1699067508888, 1366: 29.358174344462416, 1367: 44.91323697868305, 1368: 42.938661240911024, 1369: 22.216619221980583, 1370: 39.95322738436668, 1372: 42.09867491090294, 1373: 18.00679063931207, 1374: 21.02714652188713, 1376: 38.94273323006276, 1380: 17.61073111474149, 1383: 27.229915826317384, 1384: 49.68489178093504, 1386: 19.776240957434094, 1387: 35.075281995171586, 1389: 24.65153627591121, 1392: 11.644913647460342, 1394: 27.05395613190121, 1395: 52.717089100751274, 1397: 55.26671824881082, 1400: 32.448489792105406, 1408: 35.16956715065135, 1409: 20.629362990499214, 1412: 28.079266809907715, 1418: 33.26821932149288, 1419: 35.62030247771434, 1422: 26.836235799293537, 1423: 39.44574250131818, 1425: 45.210208065903906, 1428: 11.619090652814865, 1433: 19.750514441082537, 1435: 21.290145250807264, 1436: 47.54171661191838, 1437: 38.27288499112513, 1438: 22.425353458955353, 1440: 33.446191788704105, 1441: 17.68753932661955, 1445: 37.899773978681466, 1446: 48.52950084395832, 1449: 28.164607294948524, 1451: 49.966574657975364, 1453: 30.964299193641516, 1456: 34.11709874145538, 1460: 30.715941773764172, 1461: 58.87628048748538, 1466: 36.08084547613668, 1478: 14.857894239800618, 1482: 24.488649674857665, 1484: 15.339520603420839, 1485: 27.208213065646095, 1486: 41.805275675222084, 1488: 18.128902451915835, 1491: 5.4241816174643125, 1494: 62.793920340736896, 1495: 42.70848794149656, 1500: 22.294444768069706, 1508: 19.585513603965495, 1509: 21.264267078946574, 1514: 32.16860572180057, 1520: 51.15070080743851, 1523: 28.57624311887603, 1525: 50.426415424071585, 1526: 27.233037021372763, 875: 13.727379360373925, 892: 15.135496338917887, 1371: 16.998825297292292, 1513: 22.41132567838879, 1332: 4.996452363876372, 630: 2.1662592295515832, 1505: 2.5269266113670255, 1506: 2.5269266113670255, 1507: 2.5269266113670255, 728: 1.7320508075688772, 1496: 1.4142135623730951, 1175: 1.4142135623730951, 1254: 1.4142135623730951, 1259: 1.4142135623730951, 1417: 1.4142135623730951, 1010: 1.4142135623730951, 1442: 1.4142135623730951, 1012: 1.4142135623730951, 1013: 1.4142135623730951, 1019: 1.4142135623730951, 1027: 1.4142135623730951, 1129: 1.4142135623730951, 1154: 1.4142135623730951, 1333: 1.4142135623730951, 1448: 1.4142135623730951, 1490: 1.4142135623730951, 1497: 1.4142135623730951, 1499: 1.4142135623730951, 1519: 1.4142135623730951, 1011: 1.4142135623730951, 1334: 1.4142135623730951, 1008: 1.4142135623730951, 1009: 1.4142135623730951, 1261: 1.4142135623730951, 1416: 1.4142135623730951}\n"
     ]
    }
   ],
   "source": [
    "document_norm = {}\n",
    "for eachTerm in InvertedIndex[\"terms\"]:\n",
    "    for docID in InvertedIndex[\"terms\"][eachTerm]['docs']:\n",
    "        termFrequency = InvertedIndex[\"terms\"][eachTerm]['docs'][docID]['tf']\n",
    "        if docID not in document_norm:\n",
    "            document_norm[docID] = termFrequency**2\n",
    "        else:\n",
    "            document_norm[docID] += termFrequency**2\n",
    "\n",
    "document_norm = {key: math.sqrt(value) for key, value in document_norm.items()}\n",
    "print(document_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a new normalized term frequency field in the Inverted Index JSON Structure to store the Normalized Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "normalizedTermFrequencies = []\n",
    "for eachTerm in InvertedIndex[\"terms\"]:\n",
    "    for docID in InvertedIndex[\"terms\"][eachTerm]['docs']:\n",
    "        InvertedIndex[\"terms\"][eachTerm]['docs'][docID]['tf_norm'] = InvertedIndex[\"terms\"][eachTerm]['docs'][docID]['tf'] / document_norm[docID]\n",
    "        normalizedTermFrequencies.append(InvertedIndex[\"terms\"][eachTerm]['docs'][docID]['tf_norm'] )\n",
    "\n",
    "# print(str(normalizedTermFrequencies))\n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally Write the Inverted Index to a File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the final Inverted Index structure to a json file. \n",
    "\n",
    "'InvertedIndex.json'\n",
    "\n",
    "This Inverted Index will be found in the same location where this python was run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Inverted Index... In Progress\n",
      "Inverted Index File Write Completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing Inverted Index... In Progress\")\n",
    "with open(indexFilePath,'w') as invertedIndexFile:\n",
    "    invertedIndexFile.write(json.dumps(InvertedIndex))\n",
    "\n",
    "print(\"Inverted Index File Write Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input User Query Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The userQuery will be the input to the IR system\n",
    "\n",
    "The query will then be input to the top most method i.e. retrieveTop10Docs\n",
    "\n",
    "#### This will then retrieve the IDs of the Top 10 documents of the sample Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Tokens       : ['what', 'is', 'the', 'mean', 'of', 'anarch']\n",
      "Query Token Count  : {'what': 1, 'is': 1, 'the': 1, 'mean': 1, 'of': 1, 'anarch': 1}\n",
      "Query Vector : {'what': 0.34516395356044266, 'is': 0.03557615426119612, 'the': 0.03451818910486234, 'mean': 0.26814876926932557, 'of': 0.031359669860032684, 'anarch': 2.1712387562612694}\n",
      "Top 10 Documents that match the sample input query, with corresponding scores : \n",
      "12 0.1306231786481284\n",
      "1023 0.1093620818053134\n",
      "339 0.08474952699982599\n",
      "696 0.0661941066714508\n",
      "1176 0.06263976031918828\n",
      "1212 0.05136417468626282\n",
      "1158 0.05088670516230048\n",
      "643 0.04972585571201736\n",
      "1160 0.04948656879034355\n",
      "1309 0.0447406502511525\n"
     ]
    }
   ],
   "source": [
    "userQuery = \"What is the meaning of Anarchism?\"\n",
    "retrieveTop10Docs(userQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
